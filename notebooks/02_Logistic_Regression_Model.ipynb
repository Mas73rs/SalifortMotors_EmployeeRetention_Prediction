{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bf2BU6B1QmTg"
   },
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model assumptions\n",
    "\n",
    "- Outcome variable is categorical\n",
    "- Observations are independent of each other\n",
    "- No severe multicollinearity among X variables\n",
    "- No extreme outliers\n",
    "- Linear relationship between each X variable and the logit of the outcome variable\n",
    "- Sufficiently large sample size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'common_imports'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m sys\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mappend(\u001b[39m\"\u001b[39m\u001b[39m../utils\u001b[39m\u001b[39m\"\u001b[39m)  \n\u001b[1;32m      7\u001b[0m \u001b[39m# Imports everything from common_imports.py\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mcommon_imports\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m  \n\u001b[1;32m     10\u001b[0m \u001b[39m# Adds the project root to the PYTHONPATH\u001b[39;00m\n\u001b[1;32m     11\u001b[0m sys\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mappend(\u001b[39m\"\u001b[39m\u001b[39m..\u001b[39m\u001b[39m\"\u001b[39m)  \n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'common_imports'"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import sys\n",
    "\n",
    "# Adds the utils directory to the PYTHONPATH\n",
    "sys.path.append(\"../utils\")  \n",
    "\n",
    "# Imports everything from common_imports.py\n",
    "from common_imports import *  \n",
    "\n",
    "# Adds the project root to the PYTHONPATH\n",
    "sys.path.append(\"..\")  \n",
    "\n",
    "# Load cleaned dataset\n",
    "from config import cleaned_data_path\n",
    "df = pd.read_csv(cleaned_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-28T17:46:01.98927Z",
     "iopub.status.busy": "2023-09-28T17:46:01.988877Z",
     "iopub.status.idle": "2023-09-28T17:46:02.024639Z",
     "shell.execute_reply": "2023-09-28T17:46:02.023475Z",
     "shell.execute_reply.started": "2023-09-28T17:46:01.989229Z"
    }
   },
   "outputs": [],
   "source": [
    "# Copy dataframe\n",
    "df_enc = df.copy()\n",
    "\n",
    "# Encode `salary` column as ordinal numeric category\n",
    "salary_cat = ['low', 'medium', 'high']\n",
    "df_enc['salary'] = (df_enc['salary'].astype('category').cat.set_categories(salary_cat).cat.codes)\n",
    "\n",
    "# Dummy encode the `department` column\n",
    "df_enc = pd.get_dummies(df_enc, drop_first=True)\n",
    "\n",
    "# Display the new dataframe\n",
    "df_enc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Heatmap to visualize how correlated variables are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-28T17:46:34.273478Z",
     "iopub.status.busy": "2023-09-28T17:46:34.273037Z",
     "iopub.status.idle": "2023-09-28T17:46:34.700366Z",
     "shell.execute_reply": "2023-09-28T17:46:34.699015Z",
     "shell.execute_reply.started": "2023-09-28T17:46:34.273443Z"
    }
   },
   "outputs": [],
   "source": [
    "sub = ['satisfaction_level', 'last_evaluation', 'number_project', 'average_monthly_hours', 'tenure']\n",
    "fig = plt.subplots(figsize=(8, 4))\n",
    "sns.heatmap(df_enc[sub].corr(), annot=True, cmap=\"crest\")\n",
    "plt.title('Heatmap of the dataset')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Outliers:**\n",
    "\n",
    "Since logistic regression is quite sensitive to outliers, it would be a good idea at this stage to remove the outliers in the tenure column that were identified earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-28T17:47:27.302204Z",
     "iopub.status.busy": "2023-09-28T17:47:27.30177Z",
     "iopub.status.idle": "2023-09-28T17:47:27.312401Z",
     "shell.execute_reply": "2023-09-28T17:47:27.311438Z",
     "shell.execute_reply.started": "2023-09-28T17:47:27.302159Z"
    }
   },
   "outputs": [],
   "source": [
    "# Compute 25th percentile\n",
    "percent_25 = df['tenure'].quantile(0.25)\n",
    "\n",
    "# Compute 75th percentile\n",
    "percent_75 = df['tenure'].quantile(0.75)\n",
    "\n",
    "# Compute interquantile range\n",
    "iqr = percent_75 - percent_25\n",
    "\n",
    "# Define upper and lower limits for non-oulier values in column\n",
    "upper_limit = percent_75 + 1.5 * iqr\n",
    "lower_limit = percent_25 - 1.5 * iqr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-28T17:49:29.035116Z",
     "iopub.status.busy": "2023-09-28T17:49:29.034643Z",
     "iopub.status.idle": "2023-09-28T17:49:29.063997Z",
     "shell.execute_reply": "2023-09-28T17:49:29.062814Z",
     "shell.execute_reply.started": "2023-09-28T17:49:29.035082Z"
    }
   },
   "outputs": [],
   "source": [
    "# Select rows without outliers in `tenure` and save resulting dataframe in a new variable\n",
    "df_logreg = df_enc[(df_enc['tenure'] >= lower_limit) & (df_enc['tenure'] <= upper_limit)]\n",
    "\n",
    "# Display first few rows of new dataframe\n",
    "df_logreg.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Logistic Regression Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-28T17:56:10.36175Z",
     "iopub.status.busy": "2023-09-28T17:56:10.36126Z",
     "iopub.status.idle": "2023-09-28T17:56:11.189862Z",
     "shell.execute_reply": "2023-09-28T17:56:11.188244Z",
     "shell.execute_reply.started": "2023-09-28T17:56:10.361714Z"
    }
   },
   "outputs": [],
   "source": [
    "# Construct and fit a logistic model\n",
    "log_clf = LogisticRegression(random_state=42, max_iter=500).fit(X_train, y_train)\n",
    "\n",
    "# Make predictions using the model\n",
    "y_pred = log_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Confusion Matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-28T17:56:36.435534Z",
     "iopub.status.busy": "2023-09-28T17:56:36.43503Z",
     "iopub.status.idle": "2023-09-28T17:56:36.676576Z",
     "shell.execute_reply": "2023-09-28T17:56:36.675134Z",
     "shell.execute_reply.started": "2023-09-28T17:56:36.435497Z"
    }
   },
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "log_cm = confusion_matrix(y_test, y_pred, labels=log_clf.classes_)\n",
    "\n",
    "# Display for the confusion matrix\n",
    "log_disp = ConfusionMatrixDisplay(confusion_matrix=log_cm, display_labels=log_clf.classes_)\n",
    "\n",
    "# Plot confusion matrix\n",
    "log_disp.plot(values_format='')\n",
    "\n",
    "# Display plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The upper-left quadrant displays the number of true negatives. The upper-right quadrant displays the number of false positives. The bottom-left quadrant displays the number of false negatives. The bottom-right quadrant displays the number of true positives.\n",
    "\n",
    "True negatives: The number of people who did not leave that the model accurately predicted did not leave.\n",
    "\n",
    "False positives: The number of people who did not leave the model inaccurately predicted as leaving.\n",
    "\n",
    "False negatives: The number of people who left that the model inaccurately predicted did not leave\n",
    "\n",
    "True positives: The number of people who left the model accurately predicted as leaving\n",
    "\n",
    "A perfect model would yield all true negatives and true positives, and no false negatives or false positives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Imbalance Check**\n",
    "\n",
    "Checking the value counts in the `left` column. Since this is a binary classification task, the class balance informs the way you interpret accuracy metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-28T18:01:07.97434Z",
     "iopub.status.busy": "2023-09-28T18:01:07.973917Z",
     "iopub.status.idle": "2023-09-28T18:01:07.988505Z",
     "shell.execute_reply": "2023-09-28T18:01:07.98692Z",
     "shell.execute_reply.started": "2023-09-28T18:01:07.974308Z"
    }
   },
   "outputs": [],
   "source": [
    "df_logreg['left'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The split between the two classes is 83% to 17%, indicating that the data is not perfectly balanced but not too imbalanced. If the data was more severely imbalanced, resample it to achieve a more balanced distribution. However, in this instance, you can use the data without altering the class balance and proceed with model evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Metrics Report**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-28T18:08:32.356638Z",
     "iopub.status.busy": "2023-09-28T18:08:32.356122Z",
     "iopub.status.idle": "2023-09-28T18:08:32.385523Z",
     "shell.execute_reply": "2023-09-28T18:08:32.384514Z",
     "shell.execute_reply.started": "2023-09-28T18:08:32.356586Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create classification report for logistic regression model\n",
    "target_names = ['Predicted would not leave', 'Predicted would leave']\n",
    "report = classification_report(y_test, y_pred, target_names=target_names, output_dict=True)\n",
    "\n",
    "# Convert to dataframe \n",
    "report_df = pd.DataFrame(report).transpose()\n",
    "\n",
    "# Display results\n",
    "report_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to tailor the dataframe so as to be the base for the metrics that will be used in the other models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-28T18:08:56.607842Z",
     "iopub.status.busy": "2023-09-28T18:08:56.607334Z",
     "iopub.status.idle": "2023-09-28T18:08:56.637135Z",
     "shell.execute_reply": "2023-09-28T18:08:56.63525Z",
     "shell.execute_reply.started": "2023-09-28T18:08:56.607805Z"
    }
   },
   "outputs": [],
   "source": [
    "## Extracting relevant data from the report_df\n",
    "\n",
    "# Extracting the last row\n",
    "results = report_df.tail(1).reset_index(drop=True)\n",
    "\n",
    "# Rename 'f1-score' column\n",
    "results = results.rename(columns={'f1-score' : 'F1'})\n",
    "\n",
    "# Adding a new column \"model\" at the beginning with sample values\n",
    "results.insert(0, 'model', ['logistic regression'])\n",
    "\n",
    "# Dropping the last column (\"support\") from the DataFrame\n",
    "results.drop(columns=['support'], inplace=True)\n",
    "\n",
    "# Adding 'accuracy' and 'auc' columns to the DataFrame\n",
    "results['accuracy'] = report_df.loc['accuracy'][0]\n",
    "results['AUC'] = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "# Display the `result` dataframe\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table above shows that the logistic regression model achieved a precision of 79%, recall of 82%, f1-score of 80%, and accuracy of 82% with and area under the curve (AUC) of 60%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree-based Model\n",
    "\n",
    "This approach covers implementation of Decision Tree and Random Forest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model results\n",
    "\n",
    "**1. Logistic Regression**\n",
    "\n",
    "On the test set, the logistic regression model achieved the following:\n",
    "- precision : 80%, \n",
    "- recall : 83%, \n",
    "- f1-score : 80% (all weighted averages), \n",
    "- accuracy : 83%, \n",
    "- AUC (area under the curve) : 60%\n",
    "\n",
    "**2. Tree-based Machine Learning**\n",
    "\n",
    "For both models, the test results were higher than the cross-validation's.\n",
    "\n",
    "**Decision Tree**\n",
    "\n",
    "On the test set, the decision tree model achieved the following:\n",
    "- precision : 96%, \n",
    "- recall : 92%, \n",
    "- f1-score : 94% (all weighted averages), \n",
    "- accuracy : 98%, \n",
    "- AUC (area under the curve) : 96%\n",
    "\n",
    "**Random Forest**\n",
    "\n",
    "On the test set, the random forest model achieved the following:\n",
    "- precision : 98%, \n",
    "- recall : 92%, \n",
    "- f1-score : 95% (all weighted averages), \n",
    "- accuracy : 98%, \n",
    "- AUC (area under the curve) : 95%\n",
    "\n",
    "While the decision tree model has a higher AUC compared to the random forest, it can still be concluded that the random forest model is superior overall.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommendations\n",
    "\n",
    "To ensure that employees at the company are well-rested and remain satisfied, stakeholders should consider implementing the following recommendations.\n",
    "- Limit the number of projects that employees can work on. \n",
    "- Promote employees who have been with the company for at least four years or investigate why these tenured employees are dissatisfied. \n",
    "- Stakeholders could either reward employees for working longer hours or limit their hours. \n",
    "- Inform employees about the company's overtime pay policies and clarify expectations around workload and time off. Company-wide and within-team discussions can also be held to address work culture. \n",
    "- Evaluation scores should be reserved for employees who work less than 200+ hours per month, and a proportionate scale should be used to reward employees who contribute more effort."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
